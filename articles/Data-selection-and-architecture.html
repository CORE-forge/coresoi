<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="coresoi">
<title>Data selection and architecture â€¢ coresoi</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Dosis-0.4.5/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.5/font.css" rel="stylesheet">
<link href="../deps/Lora-0.4.5/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Data selection and architecture">
<meta property="og:description" content="coresoi">
<meta property="og:image" content="https://core-forge.github.io/coresoi/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">coresoi</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/Choice-of-elementary-indicators.html">Choice of elementary indicators</a>
    <a class="dropdown-item" href="../articles/Data-selection-and-architecture.html">Data selection and architecture</a>
    <a class="dropdown-item" href="../articles/try-coresoi-w-your-data.html">Try `coresoi` with you own data</a>
    <a class="dropdown-item" href="../articles/calculate-indicators.html">How to calculate indicators with `coresoi`</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Data selection and architecture</h1>
            
      
      
      <div class="d-none name"><code>Data-selection-and-architecture.Rmd</code></div>
    </div>

    
    
<p>Modern statistical approaches are currently under threat from the Big
Data era, not only because of the immense processing power required to
keep up with the amount and rate at which data is accumulating, but also
due to the wide range of sources from which data may be obtained (Arbia
et al., 2021). Most of this data, including administrative documents and
unstructured data gathered from previously unexplored sources, are
readily accessible to the public (such as crowdsourcing, open data
portals, web scraping). As a matter of fact, the web undiscloses a
wealth of intriguing material in plain (web) sight that can be
collected, aggregated and analyzed. Sometimes web platforms may operate
protection mechanisms to avoid users extracting their data. In some
other occasions data are stored in open databases and can be accessed
through APIs (Application Programming Interface) or through manual
download. This is also the case of the Banca Dati Nazionale dei
Contratti Pubblici (BNDCP) which is an open data portal offering access
information on anti- corruption, transparency, and public contracts,
handled by the Italian Anticorruption Authority (ANAC) as part of its
institutional functions. The open <a href="www.anac.gov.it/bndp">data
portal</a> is searchable by keyword and offers a series of filters to
allow users to refine their results and visualize it through a
dashboard. The data are also available in machine-readable format,
making it possible to carry out further analysis and distribute them via
API through OCDS, which is a common shared standard data model whose
goal is to support organizations and help them with transparency. The
portal currently contains more than 110 datasets which are divided into
three subsections: data on asset declarations, data on conflicts of
interest, and data on donations and sponsorships. The data on public
contracts are divided into seven subsections: data on contracting
authorities, data on economic operators, data on contracts, data on
Contracting Procedures, data on tenders, data on awarding criteria, and
data on contract prices. Utilizing this enormous quantity of data
requires a programmatic approach in addition to a strong grounding in
various web technologies. A Big Data pipeline (from now on data
pipeline) is a set of processes that ingest, clean, transform, and store
data. Data pipelines are essential for data-driven applications, as they
provide a way to reliably and efficiently move data from one place to
another. Data pipelines can be batch or real-time, depending on the
needs of the application. Batch data pipelines are typically used for
ETL (extract, transform, load) processes, where data is extracted from
one or more sources, transformed into a format that is suitable for
analysis, and then loaded into a target data store. Data pipelines can
be complex, with many different components that must work together in
order for the data to flow smoothly from one stage to the next. In order
to build a data pipeline, it is of the utmost importance to have a clear
understanding of the data flow, the scope (the data consumers
objectives) and the dependencies between the various components. In this
regard, we discuss the data pipeline for a web-based application that
ingests data from a public open data portal. The data pipeline consists
of the following components: a <strong>web scraper</strong>, a
<strong>data parsing and aggregation</strong>, a <strong>data
transformer</strong>, and a <strong>data store</strong> (i.e.Â database).
The web scraper is responsible for extracting data from the BNDCP <a href="https://dati.anticorruzione.it/#/home" class="external-link">open data portal</a>, this
happens according to a schedule which is coordinated with the updating
rate of the source. Moreover, data are actually extracted i.e.Â scraped
from 3 different BDNCP locations: the official BDNCP datasets as
mentioned above, the L190 communication default which account for
transmissions between CA (Contracting Authorities) and ANAC, finally
from a proprietary source. The latter can enrich data regarding winners
and participants of the bid, such as their number of employees, type of
company, revenues and CAE (Economic Activity Code). The information is
crucial to assess the potential risks of participating and winning on
different entities.</p>
<p><img src="assets/01-ingestion.png"></p>
<p>The parsing and aggregation step is done via AWS Glue which is a
fully-managed ETL (extract, transform, and load) service that makes it
easy to move data between data stores. It uses Apache Spark as its
underlying engine for distributed processing of data. The data parser in
AWS Glue is responsible for parsing the data extracted by the web
scraper and accounts for the file extensions the organization adopts to
replicate data across the infrastructure. By using Apache Spark, AWS
Glue can efficiently aggregate and join files for processing. Once data
are joined according to the schema ANAC provided in <a href="https://dati.anticorruzione.it/opendata/download/ManualeGestioneOperativa-OD.pdf" class="external-link">this
document</a></p>
<p><img src="assets/02-etl.png"></p>
<p>Specifically, the data transformation process is being run on Amazon
Elastic MapReduce (EMR), which is a cloud-based big data platform that
allows to process vast amounts of data using open-source tools such as
Apache Spark, Apache Hive, Apache HBase, Apache Flink, and Presto. One
of the key benefits of using a cloud-based platform like Amazon EMR is
that it allows to distribute computation across a cluster of machines,
which can help to avoid running out of memory or experiencing other
performance bottlenecks when working with big data. This means that the
data transformation process can be completed faster and more efficiently
than if it were run on a single machine or server. Additionally, because
it is only paid for the resources that are actually used. This can save
costs compared to maintaining an on-premises infrastructure.</p>
<p>The data store is the final component of the data pipeline. In order
to choose the most appropriate data store, we need to consider a number
of factors such as: capacity, performance, availability, durability,
security. In our case we decided to use Amazon Simple Storage Service
(S3). Amazon S3 is an object storage service that offers
industry-leading scalability, data availability, security, and
performance. The data store is responsible for storing the transformed
data. Finally, the aggregated data, which results in a very large file
with more than 10 millions rows and 150 columns, stays in S3 and waits
for a further transformation exploiting the same cloud means presented.
On top of this, massive indicator files (and the one resulting from the
composition of them) are calculated, generating a number of further
files. Each of these indicator file must adhere to a schema, meaning
that the outcome of the calculation must include the date of generation,
the target statistical measurement unit, among the others, and obviously
the indicator value. This needs to happen mainly for frontend
requirements purposes, so that whenever the client-side user requests
for that information, say indicator 1, the frontend has to put the
effort only into recalling the files that actually contains that
information. For each indicator, a number of files are actually
generated: one for each emergency scenario considered (Covid19, Ukraine
- Russia war, etc.) and one for each target statistical unit of
measurement (CA, winner and participants via their tax code or
geographical aggregation units, regions, municipalities). The indicator
generation process is quite straightforward: given an emergency scenario
(two are currently supported: Covid19, Ukraine - Russia war), the data
on the target units of measurement (often being referred to statistical
units: CA, winner and participants via their tax code or geographical
aggregation units, regions, municipalities), a list of indicators and
the associated calculation rules, the pipeline automatically generates
all the indicator values for all target units. The generation of
indicators is actually carried out in two steps: first, all the large
data is recollected and sorted, then stored in the local database; then,
starting from this partition, the indicators are actually calculated
according to the rules associated with each of them and stored in files,
once again in S3 spaces.</p>
<p><img src="assets/03-computation.png"></p>
<p>Thereâ€™s also a further component, the API gateway which makes
possible to directly query data from the datastore.</p>
<p>A bird-eye view of the overall infrastructure</p>
<p><img src="assets/04-overall.png"></p>
  </main>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Niccolo Salvini, Simone DelSarto, Giulio Cantone.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
